---
layout: post
title: Fine tuning Optimizations (DoRA, NEFT, LoRA+, Unsloth)
---

🚀 언어 모델 최적화: 세밀 조정 기술 향상

언어 모델의 세밀 조정 과정을 최적화하는 것은 효율성, 성능 향상 및 오버피팅 완화에 중요성을 더하고 있습니다. 최신 기술 몇 가지를 살펴보겠습니다:



- **LORA:**
  - 파라미터 전체를 조정하는 대신 어댑터를 훈련하여 언어 모델을 효율적으로 세밀 조정하는 기술.
  - 원본 가중치를 고정하고 더 작은 행렬을 훈련함으로써 업데이트해야 할 파라미터 수를 줄임 (예: 1백만 대신 16,000).
  - 개별 최적화 대신 부드럽고 균등한 업데이트를 제공함.
  - 원본 가중치를 고정하고 세밀한 행렬을 훈련하여 세밀 조정을 달성함.

- **DORA:**
  - 원본 가중치 행렬을 크기와 방향으로 분할하는 LORA의 수정 버전.
  - 원본 행렬의 크기를 조정 가능하게 하면서 방향을 훈련하는 LORA 사용.
  - 원본 행렬의 크기를 훈련하는 추가 파라미터를 추가함.
  - 크기 벡터와 방향 행렬을 더하는 방식으로 가중치 행렬을 표현함.

- **LORA Plus:**
  - 행렬 B와 A에 서로 다른 학습률을 적용하는 LORA의 수정 버전.
  - B와 A에 서로 다른 최적화 속도를 사용하여 완전한 세밀 조정에 더 가까운 성능을 달성함.

- **NEFT (Noise-Enhanced Fine-Tuning):**
  - 세밀 조정 중에 모델에 잡음을 추가하는 것으로, 특히 임베딩 레이어에 잡음을 추가함.
  - 오버피팅을 줄이고 성능을 향상시키기 위해 임베딩 레이어에 잡음을 추가함.
  - 임베딩 레이어에 잡음을 추가하여 오버피팅을 줄이고 세밀 조정 중 성능을 향상시킴.

- **UNSLOTH (Unoptimized but Speeded LOssless Tuning Helper):**
  - 세밀 조정 중에 적어도 2배의 속도 향상을 제공하는 여러 가지 속도 향상 기술의 결합.
  - Transformers 라이브러리에서 지원되며, 품질을 희생하지 않고 세밀 조정을 가속화함.
  - 세밀 조정 중에 큰 속도 향상을 제공하며, 일반적으로 약 2배의 속도 향상을 제공함.


이러한 기술들은 모두 언어 모델의 세밀 조정 과정을 최적화하기 위한 것으로, 효율성, 성능 향상, 오버피팅 감소 등을 달성하기 위한 다양한 방법을 제공함.
