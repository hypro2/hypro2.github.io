---
layout : post
title : Efficient Training of Language Models to Fill in the Middle | 논문 리뷰 영상 감상
---

1.  **대형 언어 모델 소개:**
2.   본 논문에서는 2017년에 소개된 transformer 아키텍처를 기반으로 한 대형 언어 모델의 성공에 대해 논의합니다. 이러한 모델들은 다양한 작업에서 최첨단 성능을 달성하며 읽기 이해, 질문 응답, 논리 추론 및 상식적 추론과 같은 여러 벤치마크에서 우수한 성과를 보입니다.

**[https://youtu.be/eHrUKmeQEAA?si=TaXBky4q\_1uQypJs](https://youtu.be/eHrUKmeQEAA?si=TaXBky4q_1uQypJs)**

3.  **Transformer 기반 언어 모델의 종류:**
4.  논문은 언어 모델을 인코더 전용 모델 (예: BERT), 인코더-디코더 모델 (예: T5) 및 인과 디코더 기반 모델 (예: GPT-3)로 세 가지 넓은 범주로 분류합니다. 각 클래스는 대량 언어 모델링, 스팬 예측 또는 왼쪽에서 오른쪽으로 다음 토큰 예측과 같은 다양한 목표로 훈련됩니다.
5.
6.  **인과 디코더 모델의 한계:**
7.   GPT-3와 같은 인과 디코더 기반 언어 모델은 접두사에만 조건을 걸 수 있는 한계가 있습니다. 논문에서는 접미사에 대한 조건이 필요한 상황, 특히 코드 완성과 같은 시나리오에서 이러한 제한을 탐구합니다.
8.
9.  **논문의 목표 - 중간 부분 채우기:**
10.  본 논문의 주요 목표는 인과 디코더 모델의 한계를 극복하기 위해 "중간 부분 채우기"를 가능하게 하는 것입니다. 이는 주어진 접두사와 접미사 사이의 내용을 예측하여 더 많은 맥락적이고 의미 있는 완성을 제공하는 것을 의미합니다.
11.
12.  **주요 결과 및 결론:**
13.  저자들은 훈련 데이터를 접두사, 접미사 및 중간 부분으로 세분화하여 조작합니다. 이때 모델 가이드를 위해 특수 토큰이 도입됩니다. 논문은 모델을 자기 회귀 및 중간 부분 채우기 작업 양쪽에서 훈련하는 것이 자기 회귀 성능에 부정적인 영향을 미치지 않는다는 결과를 제시합니다. 또한 중간 부분에 특화된 모델을 따로 훈련하는 것이 기존 모델을 중간 부분에 대해 fine-tuning 하는 것보다 효과적임을 보여줍니다. 데이터셋에서 중간 부분과 자기 회귀 요소의 최적 균형은 50-50 정도가 효과적인 것으로 나타납니다.

**해석:**  
이 논문은 대형 언어 모델, 훈련 목표 및 인과 디코더 모델의 한계에 대해 탐구합니다. 또한 "중간 부분 채우기" 개념을 소개하여 접미사에 대한 조건을 제공하는 동시에 모델이 주어진 접두사와 접미사 사이의 내용을 예측할 수 있게 하는 방법을 제시합니다. 저자들은 훈련 데이터를 수정하고 모델을 훈련시키기 위해 특수 토큰을 도입하는 방법을 소개하며, 효과적인 균형을 찾기 위한 다양한 실험 결과를 제시합니다.
